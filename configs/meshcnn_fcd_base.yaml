distributed_train: false
experiment_dir: './logs'
interpolation_size: [128, 128, 128]  #[91,109,91]

print_freq: 10 #frequency of showing training results on console

print_freq: 10
save_latest_freq: 10 # help='frequency of saving the latest results
save_epoch_freq: 1 # help='frequency of saving checkpoints at the end of epochs
run_test_freq: 1 # help='frequency of running test in training script
continue_train: false # load the latest model
epoch_count: 1 #'the starting epoch count, we save the model by <epoch_count>, <epoch_count>+<save_latest_freq>, ...

phase: 'train' # 'train, val, test, etc'
which_epoch: 'latest' # help='which epoch to load? set to latest to use latest cached model
niter: 100 # of iter at starting learning rate
niter_decay: 500 # of iter to linearly decay learning rate to zero
beta1: 0.9 # momentum term of adam
lr: 0.0002 # initial learning rate for adam
lr_policy: 'lambda' # learning rate policy: lambda|step|plateau
lr_decay_iters: 50 # multiply by a gamma every lr_decay_iters iterations

# data augmentation stuff
num_aug': 10 # of augmentation files
scale_verts: true # non-uniformly scale the mesh e.g., in x, y or z
slide_verts: 0 # percent vertices which will be shifted along the mesh surface
flip_edges: 0 # percent of edges to randomly flip

# tensorboard visualization
no_vis: true # will not use tensorboard
verbose_plot: true# plots network weights, etc.
is_train: True

dataroot: '..//fcd_seg'
name: 'fcd_seg'
arch: 'meshunet'
dataset_mode: 'segmentation'
ncf: [16,32,64]
ninput_edges: 17000
pool_res: [10000, 6000] 
resblocks: 1
batch_size: 1
lr: 0.003
num_aug: 20
slide_verts: 0.2

gpu_ids: 0